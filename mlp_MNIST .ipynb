{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Intro\n",
    "\n",
    "This NB is constructed such that it also runs in reasonable time on Laptop CPUs (e.g. an i3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup\n",
    "\n",
    "There are different options to set up the TensorFlow library (which now includes [Keras](https://keras.io) as backend library) on your own computer. The simplest of them is using only the CPU and can be installed in 1 command via [`conda`](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/), in an anaconda shell run:\n",
    "\n",
    "```\n",
    "conda install tensorflow\n",
    "```\n",
    "\n",
    "**NOTE**: TF migth not be compatible with your current environment, so here we create a [new environment](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) first:\n",
    "\n",
    "```\n",
    "conda create -n tf tensorflow\n",
    "conda activate tf\n",
    "```\n",
    "\n",
    "In that case you need to install jupyter, scikit-learn, matplotlib, numpy and pandas in that environment again, with e.g.:\n",
    "\n",
    "```\n",
    "conda install jupyter scikit-learn matplotlib numpy pandas\n",
    "```\n",
    "\n",
    "(If you have a [supported Nvidia graphics card](https://developer.nvidia.com/cuda-gpus) in your machine and would like to use it for accelerated network training, make sure to follow [this guide](https://www.tensorflow.org/install/gpu) to install required packages and finally use the `tensorflow-gpu` library.)\n",
    "\n",
    "The usage of the TensorFlow library in Python will be the same for CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch MNIST dataset (as done in last NB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the input data into the range [0, 1]\n",
    "## use sklearn's train_test_split to split the data into \n",
    "## 50000 instances for training (X_train, y_train), 10000 for validation (X_val, y_val) and 10000 for testing (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load an MLP classifier from sklearn with all its defaults, only specifying `random_state=42`\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "\n",
    "## train the MLP with the train set, time its execution\n",
    "\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % )\n",
    "print(\"Test set score: %f\" % )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 1\n",
    "\n",
    "1. What are the default values assumed for the MLPClassifier of sklearn?\n",
    "2. What MLP ist constructed with the defaults? \\\n",
    "   I.e. how many hidden layers and how many input, hidden and output neurons/units does the MLP have?\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now construct another MLP classifier as above but with 2 hidden layers of 100 and 50 neurons/units.\n",
    "## In addition it should used mini-batch gradient descent (mBGD) with a mini-batch size of 100\n",
    "## and train only for 100 epochs.\n",
    "## Read the docs carefully to figure out what you need to specify!\n",
    "\n",
    "## try printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n",
    "\n",
    "## train the MLP with the train set, time its execution\n",
    "\n",
    "## try again printing out the sizes of the hidden layers, the number of layers and the number of output neurons/units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the scores of the trained MLP on the train and on the test set:\n",
    "print(\"Training set score: %f\" % )\n",
    "print(\"Test set score: %f\" % )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model): # from: https://stackoverflow.com/questions/59078110/way-to-count-the-number-of-parameters-in-a-scikit-learn-model\n",
    "    \"\"\"Return total number of parameters in a \n",
    "    Scikit-Learn model.\n",
    "\n",
    "    This works for the following model types:\n",
    "     - sklearn.neural_network.MLPClassifier\n",
    "     - sklearn.neural_network.MLPRegressor\n",
    "     - sklearn.linear_model.LinearRegression\n",
    "     - and maybe some others\n",
    "    \"\"\"\n",
    "    return (sum([a.size for a in model.coefs_]) +  \n",
    "            sum([a.size for a in model.intercepts_]))\n",
    "\n",
    "## use the given function to get the number of model parameters of the last MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 2\n",
    "\n",
    "1. Does the returned number of parameters match your expectations? Write down your own calculation!\n",
    "\n",
    "\n",
    "### Answers\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now use the example from: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html\n",
    "## to plot ALL weight matrices of the first layer of the MLP trained above\n",
    "## using subplots with 20 columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test your TensorFlow installation by importing the package. The following code cell should execute without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check which computing devices TensorFlow has found on this machine. If you don't have the GPU setup on your computer, the list should just contain one CPU: `/device:CPU:0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a similar MLP as above using tf.keras, see also this [tutorial network](https://github.com/keras-team/keras/blob/fcc0bfa354c5a47625d681d0297a66ef9ff43a9e/examples/mnist_mlp.py) which also uses the MNIST dataset.\n",
    "\n",
    "Keras has a nice method `model.summary()` that prints a tabular overview of your network architecture, together with the input/output dimensions and number of parameters for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "## convert class vectors to binary class matrices\n",
    "y_train_c = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_c = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense( 50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train_c,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_c, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the training going through the epochs and in the end the trained network is evaluated on the test set. \n",
    "It shoud reache at least a classification accurary of 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now try to tune the hyper-parameters of the MLP to achieve more than 98% accuracy.\\\n",
    "List the parameters you changed to achieve this score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
